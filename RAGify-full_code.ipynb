{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This jupyter notebook helps you to build a RAG system from scratch.\n",
    "\n",
    "I strongly recommend you to checkout the [README](./readme.md) section to gain a background about this topic before diving straight into the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup dev env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Virtual Environment\n",
    "\n",
    "- [Check here](https://realpython.com/python-virtual-environments-a-primer/) why is a venv useful\n",
    "- Run cell below to create a venv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python virtual environment\n",
    "!python -m venv rag_venv\n",
    "\n",
    "# Once activated, select it as your Jupyter Kernel (see right hand top of your jupyter notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all dependencies\n",
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load API Key\n",
    "\n",
    "The Groq API key is stored in environment variables using the [python-dotenv package](https://pypi.org/project/python-dotenv/)\n",
    "\n",
    "Get your Groq key from [here](https://console.groq.com/docs/quickstart).\n",
    "Once you have the API key, create a `.env` file inside the root of the downloaded git repository.\n",
    "Add this text to the newly created file\n",
    "\n",
    "`GROQ_API_KEY=\"your_key\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "#print(os.getenv('MY_VAR'))\n",
    "print(os.getenv('GROQ_API_KEY'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run RAGify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "INFO:root:Total chunks: 28\n",
      "INFO:root:Sample chunk: # About Blunder Mifﬂin## Our HistoryFounded in 1785 by the visionary Michael Blunder, Blunder Mifﬂin...\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.46s/it]\n",
      "INFO:root:Using FlatL2 index due to small number of chunks\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.66it/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Usage Info: CompletionUsage(completion_tokens=150, prompt_tokens=1569, total_tokens=1719, completion_time=0.2, prompt_time=0.341392703, queue_time=None, total_time=0.5413927030000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Describe Blunder Mifflin's remote work policy?\n",
      "Response: According to the provided context, Blunder Mifflin's remote work policy allows for remote and hybrid work options, but they \"value in-person collaboration\" and seem to prefer traditional office work arrangements. The policy mentions that remote and hybrid work are options \"if they fit the job,\" but does not provide further details on eligibility, expectations, or procedures.\n",
      "\n",
      "It's worth noting that the context also mentions a \"Staff Grievance Procedure\" and a \"Drug, Alcohol, and Smoking Policy,\" but these sections do not directly relate to remote work policies.\n",
      "\n",
      "Overall, Blunder Mifflin's remote work policy appears to be somewhat ambiguous and in need of further clarification, as it prioritizes in-person collaboration but also offers remote options.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import groq\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Initialize the Groq client with the API key obtained from environment variables\n",
    "client = groq.Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Load a pre-trained sentence transformer model for generating embeddings\n",
    "model_name = 'all-mpnet-base-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Function to extract text content from a PDF file\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + '\\n'\n",
    "    return text\n",
    "\n",
    "# Function to split the extracted text into smaller chunks for processing\n",
    "def create_chunks(text, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# Process all PDF files in the specified directory and create chunks from their text\n",
    "pdf_directory = './input_files/'\n",
    "all_chunks = []\n",
    "for filename in os.listdir(pdf_directory):\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(pdf_directory, filename)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        chunks = create_chunks(text)\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "# Log information about chunks\n",
    "logging.info(f\"Total chunks: {len(all_chunks)}\")\n",
    "logging.info(f\"Sample chunk: {all_chunks[0][:100]}...\")  # Print first 100 characters of the first chunk\n",
    "\n",
    "# Generate embeddings for each text chunk using the sentence transformer model\n",
    "embeddings = model.encode(all_chunks)\n",
    "\n",
    "# Create a FAISS index for efficient similarity search based on embeddings\n",
    "dimension = embeddings.shape[1]\n",
    "num_chunks = len(all_chunks)\n",
    "\n",
    "# Dynamically decide on the index type based on the number of chunks\n",
    "if num_chunks < 100:\n",
    "    logging.info(\"Using FlatL2 index due to small number of chunks\")\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "else:\n",
    "    logging.info(\"Using IVFFlat index\")\n",
    "    n_clusters = min(int(np.sqrt(num_chunks)), 100)  # Adjust number of clusters based on data size\n",
    "    quantizer = faiss.IndexFlatL2(dimension)\n",
    "    index = faiss.IndexIVFFlat(quantizer, dimension, n_clusters)\n",
    "    index.train(embeddings.astype('float32'))\n",
    "\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "# Initialize cache\n",
    "cache_file = 'semantic_cache.json'\n",
    "\n",
    "# Function to load the cache from a JSON file\n",
    "def load_cache():\n",
    "    try:\n",
    "        with open(cache_file, 'r') as f:\n",
    "            cache = json.load(f)\n",
    "            if cache.get('model_name') != model_name:\n",
    "                logging.info(\"Embedding model changed. Resetting cache.\")\n",
    "                return {\"queries\": [], \"embeddings\": [], \"responses\": [], \"model_name\": model_name}\n",
    "            return cache\n",
    "    except FileNotFoundError:\n",
    "        return {\"queries\": [], \"embeddings\": [], \"responses\": [], \"model_name\": model_name}\n",
    "\n",
    "# Function to save the cache to a JSON file\n",
    "def save_cache(cache):\n",
    "    with open(cache_file, 'w') as f:\n",
    "        json.dump(cache, f)\n",
    "\n",
    "# Load the cache\n",
    "cache = load_cache()\n",
    "\n",
    "# Function to retrieve a response from the cache based on query similarity\n",
    "def retrieve_from_cache(query_embedding, threshold=0.5):\n",
    "    for i, cached_embedding in enumerate(cache['embeddings']):\n",
    "        if len(cached_embedding) != len(query_embedding):\n",
    "            logging.warning(\"Cached embedding dimension mismatch. Skipping cache entry.\")\n",
    "            continue\n",
    "        distance = np.linalg.norm(query_embedding - np.array(cached_embedding))\n",
    "        if distance < threshold:\n",
    "            return cache['responses'][i]\n",
    "    return None\n",
    "\n",
    "# Function to update the cache with a new query, embedding, and response\n",
    "def update_cache(query, query_embedding, response):\n",
    "    cache['queries'].append(query)\n",
    "    cache['embeddings'].append(query_embedding.tolist())\n",
    "    cache['responses'].append(response)\n",
    "    cache['model_name'] = model_name\n",
    "    save_cache(cache)\n",
    "\n",
    "# Function to retrieve the most relevant chunks of text based on a query\n",
    "def retrieve_relevant_chunks(query, top_k=10):\n",
    "    query_vector = model.encode([query])[0]\n",
    "\n",
    "    cached_response = retrieve_from_cache(query_vector)\n",
    "    if cached_response:\n",
    "        logging.info(\"Answer recovered from Cache.\")\n",
    "        return cached_response\n",
    "\n",
    "    top_k = min(top_k, len(all_chunks))  # Ensure we don't request more chunks than available\n",
    "    D, I = index.search(np.array([query_vector]).astype('float32'), top_k)\n",
    "    relevant_chunks = [all_chunks[i] for i in I[0]]\n",
    "\n",
    "    update_cache(query, query_vector, relevant_chunks)\n",
    "    return relevant_chunks\n",
    "\n",
    "# Function to generate a response using the Groq API based on relevant chunks\n",
    "def generate_response(query: str, relevant_chunks: List[str], model: str = \"llama-3.1-8b-instant\"):\n",
    "    context = \"\\n\".join(relevant_chunks)\n",
    "    prompt = f\"\"\"Based on the following context, please answer the question. If the answer is not fully contained in the context, provide the most relevant information available and indicate any uncertainty.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that answers questions based on the given context.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        model=model,\n",
    "        temperature=0.7,\n",
    "        max_tokens=1024,\n",
    "        top_p=1,\n",
    "        stream=False,\n",
    "        stop=None\n",
    "    )\n",
    "\n",
    "    response = chat_completion.choices[0].message.content.strip()\n",
    "    usage_info = chat_completion.usage\n",
    "    logging.info(f\"Usage Info: {usage_info}\")\n",
    "    return response, usage_info\n",
    "\n",
    "# Function to process a query using retrieval-augmented generation (RAG)\n",
    "def rag_query(query: str, top_k: int = 10) -> str:\n",
    "    relevant_chunks = retrieve_relevant_chunks(query, top_k)\n",
    "    response, usage_info = generate_response(query, relevant_chunks)\n",
    "    return response\n",
    "\n",
    "# Test the RAG-based system with a sample query\n",
    "test_query = \"Describe Blunder Mifflin's remote work policy?\"\n",
    "result = rag_query(test_query)\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Response: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full json response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-8daa49bf-3a07-42fb-ab0e-c954a10d0107\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1722858859,\n",
      "  \"model\": \"gemma2-9b-it\",\n",
      "  \"system_fingerprint\": \"fp_10c08bf97d\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"I'm reading a book about anti-gravity. It's impossible to put down!  \\ud83d\\udcda\\ud83d\\ude04  \\n\\n\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"logprobs\": null\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 28,\n",
      "    \"completion_tokens\": 27,\n",
      "    \"total_tokens\": 55,\n",
      "    \"prompt_time\": 0.003,\n",
      "    \"completion_time\": 0.054,\n",
      "    \"total_time\": 0.057\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from groq import Groq\n",
    "from datetime import datetime\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Give me a funny one-liner.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gemma2-9b-it\",  # gemma2-9b-it\n",
    "    temperature=1,\n",
    "    max_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=False,\n",
    "    stop=None\n",
    ")\n",
    "\n",
    "# Create a dictionary with the desired structure\n",
    "response_dict = {\n",
    "    \"id\": chat_completion.id,\n",
    "    \"object\": \"chat.completion\",\n",
    "    \"created\": int(datetime.now().timestamp()),\n",
    "    \"model\": chat_completion.model,\n",
    "    \"system_fingerprint\": chat_completion.system_fingerprint,  # This might be None\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"index\": 0,\n",
    "            \"message\": {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": chat_completion.choices[0].message.content\n",
    "            },\n",
    "            \"finish_reason\": chat_completion.choices[0].finish_reason,\n",
    "            \"logprobs\": None\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\n",
    "        \"prompt_tokens\": chat_completion.usage.prompt_tokens,\n",
    "        \"completion_tokens\": chat_completion.usage.completion_tokens,\n",
    "        \"total_tokens\": chat_completion.usage.total_tokens,\n",
    "        \"prompt_time\": round(chat_completion.usage.prompt_time, 3),\n",
    "        \"completion_time\": round(chat_completion.usage.completion_time, 3),\n",
    "        \"total_time\": round(chat_completion.usage.total_time, 3)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print the formatted JSON response\n",
    "print(json.dumps(response_dict, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rate limits\n",
    "\n",
    "- [rate limits](https://console.groq.com/docs/rate-limits)\n",
    "- [errors](https://console.groq.com/docs/errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Usage:\n",
      "  Limit: 131072 tokens per minute\n",
      "  Remaining: 131063 tokens\n",
      "  Resets in: 0.00 seconds\n",
      "  Resets at: 2024-08-05 14:21:34\n",
      "\n",
      "Daily Request Limits:\n",
      "  Limit: 14400 requests per day\n",
      "  Remaining: 14399 requests\n",
      "  Resets in: 6.00 seconds\n",
      "  Resets at: 2024-08-05 14:21:40\n",
      "\n",
      "This request used:\n",
      "  Prompt tokens: 16\n",
      "  Completion tokens: 50\n",
      "  Total tokens: 66\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set your Groq API key\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "# Groq API endpoint\n",
    "# https://console.groq.com/docs/api-reference#chat-create\n",
    "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "\n",
    "# Headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Example request payload\n",
    "payload = {\n",
    "    \"model\": \"llama-3.1-8b-instant\", #llama-3.1-8b-instant, gemma2-9b-it\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "}\n",
    "\n",
    "def parse_time(time_str):\n",
    "    if time_str.endswith('ms'):\n",
    "        return float(time_str[:-2]) / 1000  # Convert milliseconds to seconds\n",
    "    elif time_str.endswith('s'):\n",
    "        return float(time_str[:-1])\n",
    "    else:\n",
    "        try:\n",
    "            return float(time_str)  # Assume it's already in seconds\n",
    "        except ValueError:\n",
    "            return 0  # Default to 0 if format is unrecognized\n",
    "\n",
    "def check_rate_limits():\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Token usage limits\n",
    "        token_limit = int(response.headers.get('x-ratelimit-limit-tokens', 0))\n",
    "        tokens_remaining = int(response.headers.get('x-ratelimit-remaining-tokens', 0))\n",
    "        token_reset = parse_time(response.headers.get('x-ratelimit-reset-tokens', '0'))\n",
    "\n",
    "        # Daily request limits\n",
    "        daily_limit = int(response.headers.get('x-ratelimit-limit-requests', 0))\n",
    "        requests_remaining = int(response.headers.get('x-ratelimit-remaining-requests', 0))\n",
    "        request_reset = parse_time(response.headers.get('x-ratelimit-reset-requests', '0'))\n",
    "\n",
    "        # Calculate reset times\n",
    "        token_reset_time = datetime.now() + timedelta(seconds=token_reset)\n",
    "        request_reset_time = datetime.now() + timedelta(seconds=request_reset)\n",
    "\n",
    "        print(f\"Token Usage:\")\n",
    "        print(f\"  Limit: {token_limit} tokens per minute\")\n",
    "        print(f\"  Remaining: {tokens_remaining} tokens\")\n",
    "        print(f\"  Resets in: {token_reset:.2f} seconds\")\n",
    "        print(f\"  Resets at: {token_reset_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"\\nDaily Request Limits:\")\n",
    "        print(f\"  Limit: {daily_limit} requests per day\")\n",
    "        print(f\"  Remaining: {requests_remaining} requests\")\n",
    "        print(f\"  Resets in: {request_reset:.2f} seconds\")\n",
    "        print(f\"  Resets at: {request_reset_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "        # Check usage for this specific request\n",
    "        usage = response.json().get('usage', {})\n",
    "        print(\"\\nThis request used:\")\n",
    "        print(f\"  Prompt tokens: {usage.get('prompt_tokens', 0)}\")\n",
    "        print(f\"  Completion tokens: {usage.get('completion_tokens', 0)}\")\n",
    "        print(f\"  Total tokens: {usage.get('total_tokens', 0)}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_rate_limits()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
