{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This jupyter notebook helps you to build a RAG system from scratch.\n",
    "\n",
    "I strongly recommend you to checkout the [README](./readme.md) section to gain a background about this topic before diving straight into the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup dev env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Virtual Environment\n",
    "\n",
    "- [Check here](https://realpython.com/python-virtual-environments-a-primer/) why is a venv useful\n",
    "- Run cell below to create a venv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python virtual environment\n",
    "!python -m venv rag_venv\n",
    "\n",
    "# Once activated, select it as your Jupyter Kernel (see right hand top of your jupyter notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all dependencies\n",
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load API Key\n",
    "\n",
    "The Groq API key is stored in environment variables using the [python-dotenv package](https://pypi.org/project/python-dotenv/)\n",
    "\n",
    "Get your Groq key from [here](https://console.groq.com/docs/quickstart).\n",
    "Once you have the API key, create a `.env` file inside the root of the downloaded git repository.\n",
    "Add this text to the newly created file\n",
    "\n",
    "`GROQ_API_KEY=\"your_key\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "#print(os.getenv('MY_VAR'))\n",
    "print(os.getenv('GROQ_API_KEY'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run RAGify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-mpnet-base-v2\n",
      "INFO:root:Total chunks: 28\n",
      "INFO:root:Sample chunk: # About Blunder Mifﬂin## Our HistoryFounded in 1785 by the visionary Michael Blunder, Blunder Mifﬂin...\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.46s/it]\n",
      "INFO:root:Using FlatL2 index due to small number of chunks\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.66it/s]\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Usage Info: CompletionUsage(completion_tokens=150, prompt_tokens=1569, total_tokens=1719, completion_time=0.2, prompt_time=0.341392703, queue_time=None, total_time=0.5413927030000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Describe Blunder Mifflin's remote work policy?\n",
      "Response: According to the provided context, Blunder Mifflin's remote work policy allows for remote and hybrid work options, but they \"value in-person collaboration\" and seem to prefer traditional office work arrangements. The policy mentions that remote and hybrid work are options \"if they fit the job,\" but does not provide further details on eligibility, expectations, or procedures.\n",
      "\n",
      "It's worth noting that the context also mentions a \"Staff Grievance Procedure\" and a \"Drug, Alcohol, and Smoking Policy,\" but these sections do not directly relate to remote work policies.\n",
      "\n",
      "Overall, Blunder Mifflin's remote work policy appears to be somewhat ambiguous and in need of further clarification, as it prioritizes in-person collaboration but also offers remote options.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import groq\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import json\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "import hashlib\n",
    "import time\n",
    "from groq import RateLimitError\n",
    "\n",
    "# I have loaded environment variables to keep sensitive information out of the codebase.\n",
    "# This is crucial for security and allows for easy configuration changes across environments.\n",
    "load_dotenv()\n",
    "\n",
    "# I have set up logging to track execution and debug issues.\n",
    "# Proper logging is essential for monitoring and troubleshooting in production environments.\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# I have initialized the Groq client for API access.\n",
    "# Here, I'm using an API key stored in environment variables for security.\n",
    "# The commented out line shows an alternative using Streamlit secrets, which is useful for deployment scenarios.\n",
    "client = groq.Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "    #api_key=st.secrets[\"GROQ_API_KEY\"], # GROQ_API_KEY = \"\"\n",
    ")\n",
    "\n",
    "# I have loaded a pre-trained sentence transformer model for generating text embeddings.\n",
    "# I chose 'all-mpnet-base-v2' for its balance of performance and accuracy.\n",
    "# This model is crucial for converting text to vector representations for similarity search.\n",
    "model_name = 'all-mpnet-base-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # I have extracted text from PDFs to make the content searchable.\n",
    "    # This allows us to work with various document formats in a unified way.\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + '\\n'\n",
    "    return text\n",
    "\n",
    "def create_chunks(text, chunk_size=1000, chunk_overlap=200):\n",
    "    # I have chunked the text for two main reasons:\n",
    "    # 1. It allows us to process long documents that might exceed model token limits.\n",
    "    # 2. It creates more granular pieces of text, improving retrieval accuracy.\n",
    "    # I have used overlap to maintain context between chunks.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "def get_files_hash(directory):\n",
    "    # I have hashed the input files to detect changes.\n",
    "    # This is crucial for maintaining an up-to-date knowledge base without unnecessary reprocessing.\n",
    "    hash_md5 = hashlib.md5()\n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        if filename.endswith('.pdf'):\n",
    "            with open(os.path.join(directory, filename), \"rb\") as f:\n",
    "                for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                    hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "@st.cache_data\n",
    "def process_pdfs(_hash=None):\n",
    "    # I have used caching to avoid reprocessing PDFs on every run.\n",
    "    # This significantly improves performance for repeated queries.\n",
    "    pdf_directory = './input_files/'\n",
    "    current_hash = get_files_hash(pdf_directory)\n",
    "\n",
    "    # I have cleared the cache if input files have changed.\n",
    "    # This ensures we're always working with the most up-to-date information.\n",
    "    if _hash is not None and _hash != current_hash:\n",
    "        st.cache_data.clear()\n",
    "\n",
    "    all_chunks = []\n",
    "    chunk_to_doc = {}\n",
    "    for filename in os.listdir(pdf_directory):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_directory, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            chunks = create_chunks(text)\n",
    "            all_chunks.extend(chunks)\n",
    "            for chunk in chunks:\n",
    "                chunk_to_doc[chunk] = filename\n",
    "\n",
    "    # I have used logging to help with debugging and monitoring the chunking process.\n",
    "    logging.info(f\"Total chunks: {len(all_chunks)}\")\n",
    "    logging.info(f\"Sample chunk: {all_chunks[0][:100]}...\")\n",
    "\n",
    "    return all_chunks, chunk_to_doc, current_hash\n",
    "\n",
    "@st.cache_resource\n",
    "def create_faiss_index(all_chunks):\n",
    "    # I have used FAISS for efficient similarity search.\n",
    "    # This is crucial for quickly finding relevant chunks when answering queries.\n",
    "    embeddings = model.encode(all_chunks)\n",
    "    dimension = embeddings.shape[1]\n",
    "    num_chunks = len(all_chunks)\n",
    "\n",
    "    # I have dynamically chosen the index type based on the dataset size.\n",
    "    # This optimizes search performance: FlatL2 for small datasets, IVFFlat for larger ones.\n",
    "    if num_chunks < 100:\n",
    "        logging.info(\"Using FlatL2 index due to small number of chunks\")\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "    else:\n",
    "        logging.info(\"Using IVFFlat index\")\n",
    "        n_clusters = min(int(np.sqrt(num_chunks)), 100)  # Balancing clustering and search efficiency\n",
    "        quantizer = faiss.IndexFlatL2(dimension)\n",
    "        index = faiss.IndexIVFFlat(quantizer, dimension, n_clusters)\n",
    "        index.train(embeddings.astype('float32'))\n",
    "\n",
    "    index.add(embeddings.astype('float32'))\n",
    "    return index\n",
    "\n",
    "# I have initialized a cache for storing query results.\n",
    "# Caching improves response times for repeated or similar queries.\n",
    "cache_file = 'semantic_cache.json'\n",
    "\n",
    "def load_cache():\n",
    "    # I have loaded the cache from a file to persist it across sessions.\n",
    "    # This improves the system's efficiency over time.\n",
    "    try:\n",
    "        with open(cache_file, 'r') as f:\n",
    "            cache = json.load(f)\n",
    "            # I have reset the cache if the embedding model changes to ensure consistency.\n",
    "            if cache.get('model_name') != model_name:\n",
    "                logging.info(\"Embedding model changed. Resetting cache.\")\n",
    "                return {\"queries\": [], \"embeddings\": [], \"responses\": [], \"model_name\": model_name}\n",
    "            return cache\n",
    "    except FileNotFoundError:\n",
    "        return {\"queries\": [], \"embeddings\": [], \"responses\": [], \"model_name\": model_name}\n",
    "\n",
    "def save_cache(cache):\n",
    "    # I have regularly saved the cache to ensure we don't lose valuable precomputed results.\n",
    "    with open(cache_file, 'w') as f:\n",
    "        json.dump(cache, f)\n",
    "\n",
    "cache = load_cache()\n",
    "\n",
    "def retrieve_from_cache(query_embedding, threshold=0.5):\n",
    "    # I have implemented semantic caching to reuse results for similar queries.\n",
    "    # This significantly reduces API calls and improves response times.\n",
    "    for i, cached_embedding in enumerate(cache['embeddings']):\n",
    "        if len(cached_embedding) != len(query_embedding):\n",
    "            logging.warning(\"Cached embedding dimension mismatch. Skipping cache entry.\")\n",
    "            continue\n",
    "        distance = np.linalg.norm(query_embedding - np.array(cached_embedding))\n",
    "        if distance < threshold:\n",
    "            return cache['responses'][i]\n",
    "    return None\n",
    "\n",
    "def update_cache(query, query_embedding, response):\n",
    "    # I have updated the cache with new queries to continually improve performance.\n",
    "    cache['queries'].append(query)\n",
    "    cache['embeddings'].append(query_embedding.tolist())\n",
    "    cache['responses'].append(response)\n",
    "    cache['model_name'] = model_name\n",
    "    save_cache(cache)\n",
    "\n",
    "def retrieve_relevant_chunks(query, index, all_chunks, top_k=10):\n",
    "    # I have used vector similarity to find the most relevant chunks.\n",
    "    # This is more effective than keyword matching for understanding context and semantics.\n",
    "    query_vector = model.encode([query])[0]\n",
    "\n",
    "    cached_response = retrieve_from_cache(query_vector)\n",
    "    if cached_response:\n",
    "        logging.info(\"Answer recovered from Cache.\")\n",
    "        return cached_response\n",
    "\n",
    "    # I have limited top_k to avoid retrieving more chunks than available.\n",
    "    top_k = min(top_k, len(all_chunks))\n",
    "    D, I = index.search(np.array([query_vector]).astype('float32'), top_k)\n",
    "    relevant_chunks = [all_chunks[i] for i in I[0]]\n",
    "\n",
    "    update_cache(query, query_vector, relevant_chunks)\n",
    "    return relevant_chunks\n",
    "\n",
    "def generate_response(query: str, relevant_chunks: List[str], primary_model: str = \"llama-3.1-8b-instant\", fallback_model: str = \"gemma2-9b-it\", max_retries: int = 3):\n",
    "    # I have used a language model to generate responses based on retrieved chunks.\n",
    "    # This allows for more natural and contextually appropriate answers.\n",
    "    context = \"\\n\".join(relevant_chunks)\n",
    "    prompt = f\"\"\"Based on the following context, please answer the question. If the answer is not fully contained in the context, provide the most relevant information available and indicate any uncertainty.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # I have implemented a fallback mechanism and retry logic for robustness.\n",
    "    # This ensures the system can handle API errors and rate limits gracefully.\n",
    "    models = [primary_model, fallback_model]\n",
    "    for model in models:\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                chat_completion = client.chat.completions.create(\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"You are a helpful assistant that answers questions based on the given context.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt\n",
    "                        }\n",
    "                    ],\n",
    "                    model=model,\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=1024,\n",
    "                    top_p=1,\n",
    "                    stream=False,\n",
    "                    stop=None\n",
    "                )\n",
    "\n",
    "                response = chat_completion.choices[0].message.content.strip()\n",
    "                usage_info = {\n",
    "                    \"prompt_tokens\": chat_completion.usage.prompt_tokens,\n",
    "                    \"completion_tokens\": chat_completion.usage.completion_tokens,\n",
    "                    \"total_tokens\": chat_completion.usage.total_tokens,\n",
    "                    \"model_used\": model\n",
    "                }\n",
    "                logging.info(f\"Usage Info: {usage_info}\")\n",
    "                return response, usage_info, relevant_chunks\n",
    "\n",
    "            except RateLimitError as e:\n",
    "                if model == fallback_model and attempt == max_retries - 1:\n",
    "                    logging.error(f\"Rate limit exceeded for both models after {max_retries} attempts.\")\n",
    "                    raise e\n",
    "                logging.warning(f\"Rate limit exceeded for model {model}. Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error occurred with model {model}: {str(e)}\")\n",
    "                break  # Move to the next model if any other error occurs\n",
    "\n",
    "    raise Exception(\"Failed to generate response with all available models.\")\n",
    "\n",
    "def rag_query(query: str, index, all_chunks, chunk_to_doc, top_k: int = 10) -> tuple:\n",
    "    # I have combined retrieval and generation for a complete RAG pipeline.\n",
    "    # RAG allows us to ground the model's responses in specific, relevant information.\n",
    "    relevant_chunks = retrieve_relevant_chunks(query, index, all_chunks, top_k)\n",
    "    response, usage_info, used_chunks = generate_response(query, relevant_chunks)\n",
    "\n",
    "    # I have tracked source documents for transparency and citation.\n",
    "    source_docs = list(set([chunk_to_doc.get(chunk, \"Unknown Source\") for chunk in used_chunks]))\n",
    "\n",
    "    return response, usage_info, source_docs\n",
    "\n",
    "# I have configured the Streamlit app.\n",
    "# I have used Streamlit for rapid prototyping and easy deployment of the user interface.\n",
    "st.set_page_config(page_title=\"Blunder Mifflin\", page_icon=\":soccer:\", layout=\"wide\", initial_sidebar_state=\"expanded\", menu_items=None)\n",
    "\n",
    "def main():\n",
    "    st.write(\"Ask questions about Blunder Mifflin's Company Policy.\")\n",
    "\n",
    "    # I have processed PDFs and created the index at the start to ensure up-to-date information.\n",
    "    all_chunks, chunk_to_doc, current_hash = process_pdfs()\n",
    "    index = create_faiss_index(all_chunks)\n",
    "\n",
    "    # I have provided default questions to guide users and demonstrate system capabilities.\n",
    "    default_questions = [\n",
    "        \"Select a question\",\n",
    "        \"What is Blunder Mifflin's product range?\",\n",
    "        \"Who is part of Blunder Mifflin's team?\",\n",
    "        \"What is Blunder Mifflin's policy relationships and nepotism?\",\n",
    "        \"Describe Blunder Mifflin's Birthday Party Committee Rules\",\n",
    "        \"Other (Type your own question)\"\n",
    "    ]\n",
    "\n",
    "    # I have used a dropdown for ease of use, but also allowed custom questions for flexibility.\n",
    "    selected_question = st.selectbox(\"Choose a question or select 'Other' to type your own:\", default_questions)\n",
    "\n",
    "    if selected_question == \"Other (Type your own question)\":\n",
    "        user_query = st.text_input(\"Enter your question:\")\n",
    "    elif selected_question != \"Select a question\":\n",
    "        user_query = selected_question\n",
    "    else:\n",
    "        user_query = \"\"\n",
    "\n",
    "    if user_query:\n",
    "        pass\n",
    "\n",
    "    # I have used a button to trigger the query process, giving users control over when to send a request.\n",
    "    if st.button(\"Get Answer\"):\n",
    "        if user_query and user_query != \"Select a question\":\n",
    "            with st.spinner(\"Generating answer...\"):\n",
    "                # I have rechecked for changes in PDFs to ensure we're using the latest data.\n",
    "                all_chunks, chunk_to_doc, _ = process_pdfs(current_hash)\n",
    "                index = create_faiss_index(all_chunks)\n",
    "                response, usage_info, source_docs = rag_query(user_query, index, all_chunks, chunk_to_doc)\n",
    "\n",
    "            # I have displayed the response, sources, and usage info for transparency.\n",
    "            st.subheader(\"Answer:\")\n",
    "            st.write(response)\n",
    "\n",
    "            st.subheader(\"Source Documents:\")\n",
    "            for doc in source_docs:\n",
    "                st.write(f\"- {doc}\")\n",
    "\n",
    "            with st.expander(\"Usage Information\"):\n",
    "                st.json({\n",
    "                    \"Prompt Tokens\": usage_info[\"prompt_tokens\"],\n",
    "                    \"Completion Tokens\": usage_info[\"completion_tokens\"],\n",
    "                    \"Total Tokens\": usage_info[\"total_tokens\"],\n",
    "                    \"Model Used\": usage_info[\"model_used\"]\n",
    "                })\n",
    "        else:\n",
    "            st.warning(\"Please select a question or enter your own.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full json response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-8daa49bf-3a07-42fb-ab0e-c954a10d0107\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1722858859,\n",
      "  \"model\": \"gemma2-9b-it\",\n",
      "  \"system_fingerprint\": \"fp_10c08bf97d\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"I'm reading a book about anti-gravity. It's impossible to put down!  \\ud83d\\udcda\\ud83d\\ude04  \\n\\n\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"logprobs\": null\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 28,\n",
      "    \"completion_tokens\": 27,\n",
      "    \"total_tokens\": 55,\n",
      "    \"prompt_time\": 0.003,\n",
      "    \"completion_time\": 0.054,\n",
      "    \"total_time\": 0.057\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from groq import Groq\n",
    "from datetime import datetime\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Give me a funny one-liner.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gemma2-9b-it\",  # gemma2-9b-it\n",
    "    temperature=1,\n",
    "    max_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=False,\n",
    "    stop=None\n",
    ")\n",
    "\n",
    "# Create a dictionary with the desired structure\n",
    "response_dict = {\n",
    "    \"id\": chat_completion.id,\n",
    "    \"object\": \"chat.completion\",\n",
    "    \"created\": int(datetime.now().timestamp()),\n",
    "    \"model\": chat_completion.model,\n",
    "    \"system_fingerprint\": chat_completion.system_fingerprint,  # This might be None\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"index\": 0,\n",
    "            \"message\": {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": chat_completion.choices[0].message.content\n",
    "            },\n",
    "            \"finish_reason\": chat_completion.choices[0].finish_reason,\n",
    "            \"logprobs\": None\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\n",
    "        \"prompt_tokens\": chat_completion.usage.prompt_tokens,\n",
    "        \"completion_tokens\": chat_completion.usage.completion_tokens,\n",
    "        \"total_tokens\": chat_completion.usage.total_tokens,\n",
    "        \"prompt_time\": round(chat_completion.usage.prompt_time, 3),\n",
    "        \"completion_time\": round(chat_completion.usage.completion_time, 3),\n",
    "        \"total_time\": round(chat_completion.usage.total_time, 3)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print the formatted JSON response\n",
    "print(json.dumps(response_dict, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rate limits\n",
    "\n",
    "- [rate limits](https://console.groq.com/docs/rate-limits)\n",
    "- [errors](https://console.groq.com/docs/errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Usage:\n",
      "  Limit: 131072 tokens per minute\n",
      "  Remaining: 131063 tokens\n",
      "  Resets in: 0.00 seconds\n",
      "  Resets at: 2024-08-05 14:21:34\n",
      "\n",
      "Daily Request Limits:\n",
      "  Limit: 14400 requests per day\n",
      "  Remaining: 14399 requests\n",
      "  Resets in: 6.00 seconds\n",
      "  Resets at: 2024-08-05 14:21:40\n",
      "\n",
      "This request used:\n",
      "  Prompt tokens: 16\n",
      "  Completion tokens: 50\n",
      "  Total tokens: 66\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set your Groq API key\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "# Groq API endpoint\n",
    "# https://console.groq.com/docs/api-reference#chat-create\n",
    "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "\n",
    "# Headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Example request payload\n",
    "payload = {\n",
    "    \"model\": \"llama-3.1-8b-instant\", #llama-3.1-8b-instant, gemma2-9b-it\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "}\n",
    "\n",
    "def parse_time(time_str):\n",
    "    if time_str.endswith('ms'):\n",
    "        return float(time_str[:-2]) / 1000  # Convert milliseconds to seconds\n",
    "    elif time_str.endswith('s'):\n",
    "        return float(time_str[:-1])\n",
    "    else:\n",
    "        try:\n",
    "            return float(time_str)  # Assume it's already in seconds\n",
    "        except ValueError:\n",
    "            return 0  # Default to 0 if format is unrecognized\n",
    "\n",
    "def check_rate_limits():\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Token usage limits\n",
    "        token_limit = int(response.headers.get('x-ratelimit-limit-tokens', 0))\n",
    "        tokens_remaining = int(response.headers.get('x-ratelimit-remaining-tokens', 0))\n",
    "        token_reset = parse_time(response.headers.get('x-ratelimit-reset-tokens', '0'))\n",
    "\n",
    "        # Daily request limits\n",
    "        daily_limit = int(response.headers.get('x-ratelimit-limit-requests', 0))\n",
    "        requests_remaining = int(response.headers.get('x-ratelimit-remaining-requests', 0))\n",
    "        request_reset = parse_time(response.headers.get('x-ratelimit-reset-requests', '0'))\n",
    "\n",
    "        # Calculate reset times\n",
    "        token_reset_time = datetime.now() + timedelta(seconds=token_reset)\n",
    "        request_reset_time = datetime.now() + timedelta(seconds=request_reset)\n",
    "\n",
    "        print(f\"Token Usage:\")\n",
    "        print(f\"  Limit: {token_limit} tokens per minute\")\n",
    "        print(f\"  Remaining: {tokens_remaining} tokens\")\n",
    "        print(f\"  Resets in: {token_reset:.2f} seconds\")\n",
    "        print(f\"  Resets at: {token_reset_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"\\nDaily Request Limits:\")\n",
    "        print(f\"  Limit: {daily_limit} requests per day\")\n",
    "        print(f\"  Remaining: {requests_remaining} requests\")\n",
    "        print(f\"  Resets in: {request_reset:.2f} seconds\")\n",
    "        print(f\"  Resets at: {request_reset_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "        # Check usage for this specific request\n",
    "        usage = response.json().get('usage', {})\n",
    "        print(\"\\nThis request used:\")\n",
    "        print(f\"  Prompt tokens: {usage.get('prompt_tokens', 0)}\")\n",
    "        print(f\"  Completion tokens: {usage.get('completion_tokens', 0)}\")\n",
    "        print(f\"  Total tokens: {usage.get('total_tokens', 0)}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_rate_limits()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
